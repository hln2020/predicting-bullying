{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"A3Rx5bOToH8f"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","import time\n","import itertools\n","import math\n","import os\n","import threading\n","import concurrent.futures\n","import numba as nb\n","import multiprocessing as mp\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n","from sklearn.metrics import roc_auc_score\n","from collections import Counter\n","from concurrent.futures import ThreadPoolExecutor\n","from concurrent.futures import ProcessPoolExecutor\n","from concurrent.futures import wait\n","from concurrent.futures import as_completed\n","from joblib import Parallel, delayed\n","from numba import njit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pR4Vz03RoQUS"},"outputs":[],"source":["# Load in data\n","df = pd.read_csv('cleaned_bullying.csv')\n","\n","# Drop columns\n","df = df.drop(columns=['Unnamed: 0',\n","                      'Bullied_on_school_property_in_past_12_months',\n","                      'Bullied_not_on_school_property_in_past_12_months',\n","                      'Cyber_bullied_in_past_12_months'])\n","\n","# Split into X and y\n","X = df.drop('bullied', axis=1).values\n","y = df['bullied'].values\n","\n","# Load copy of dataframe\n","data = df\n","\n","# Train/test split\n","train_data = data.sample(frac=0.7, random_state=1)\n","test_data = data.drop(train_data.index)"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1682715969414,"user":{"displayName":"Alex Herron","userId":"01849162412288078755"},"user_tz":240},"id":"6RS4HEg3ptuQ"},"source":["# 1. Decision Tree Model from Scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmkQGIP0wsJe"},"outputs":[],"source":["def generate_possible_splits(data):\n","    # generate all possible binary splits of the data\n","    features = list(data.columns)[:-1]\n","    splits = []\n","    for feature in features:\n","        values = data[feature].unique()\n","        for i in range(1, len(values)):\n","            for split in itertools.combinations(values, i):\n","                splits.append((feature, split))\n","    return splits\n","\n","def evaluate_split(split, data):\n","    # evaluate a split by calculating the information gain\n","    feature, values = split\n","    total = len(data)\n","    left = data[data[feature].isin(values)]\n","    right = data[~data[feature].isin(values)]\n","    left_proportion = len(left) / total\n","    right_proportion = len(right) / total\n","    entropy_before = calculate_entropy(data)\n","    entropy_after = (left_proportion * calculate_entropy(left) +\n","                     right_proportion * calculate_entropy(right))\n","    information_gain = entropy_before - entropy_after\n","    return information_gain, left, right\n","\n","def calculate_entropy(data):\n","    # calculate the entropy of a set of data\n","    total = len(data)\n","    counts = data.iloc[:, -1].value_counts()\n","    entropy = 0\n","    for count in counts:\n","        proportion = count / total\n","        entropy -= proportion * math.log2(proportion)\n","    return entropy\n","\n","def build_decision_tree(data):\n","    # build a decision tree using a brute force algorithm\n","    if len(data.iloc[:, -1].unique()) == 1:\n","        # all instances have the same class\n","        return {'class': data.iloc[0, -1]}\n","    best_split = None\n","    best_gain = 0\n","    for split in generate_possible_splits(data):\n","        gain, left, right = evaluate_split(split, data)\n","        if gain > best_gain:\n","            best_gain = gain\n","            best_split = split\n","            best_left = left\n","            best_right = right\n","    if best_split is None:\n","        # no split improved information gain\n","        return {'class': data.iloc[:, -1].value_counts().idxmax()}\n","    else:\n","        left_subtree = build_decision_tree(best_left)\n","        right_subtree = build_decision_tree(best_right)\n","        return {'feature': best_split[0],\n","                'value': best_split[1],\n","                'left': left_subtree,\n","                'right': right_subtree}\n","\n","def predict(instance, tree):\n","    if 'class' in tree:\n","        # leaf node, return class\n","        return tree['class']\n","    else:\n","        feature = tree['feature']\n","        value = tree['value']\n","        if instance[feature] in value:\n","            subtree = tree['left']\n","        else:\n","            subtree = tree['right']\n","        return predict(instance, subtree)\n","    \n","def evaluate():\n","    # load dataset\n","    data = df\n","\n","    # split data into training and test sets\n","    train_data = data.sample(frac=0.7, random_state=1)\n","    test_data = data.drop(train_data.index)\n","\n","    # train decision tree\n","    start = time.time()\n","    tree = build_decision_tree(train_data)\n","    end = time.time()\n","    time_diff = end - start\n","\n","    # evaluate decision tree\n","    y_true = test_data.iloc[:, -1]\n","    y_pred = test_data.apply(lambda x: predict(x, tree), axis=1)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred))\n","\n","    # print results\n","    print(f'AUC: {auc}')\n","    print(f'Accuracy: {accuracy}')\n","    print(f'Time: {time_diff}')\n","    \n","    # Make beeping sounds to notify end of evaluation\n","    beep = lambda x: os.system(\"echo '\\a';sleep 0.5;\" * x)\n","    beep(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KQ_yMU3zsJ5","outputId":"00021791-f32b-4853-be5d-353073eb27ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.5807639826418268\n","Accuracy: 0.6037850419997975\n","Time: 273.81109714508057\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n"]}],"source":["evaluate()"]},{"cell_type":"markdown","metadata":{"id":"7-Uh7GUzzsJ6"},"source":["# 2. Evalute Decision Tree Model Using Line Profiler\n","\n","~96% of time spent on evaluate_split(split, data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoT3TmjVzsJ6"},"outputs":[],"source":["%load_ext line_profiler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kuuwln2zsJ7"},"outputs":[],"source":["%lprun -f build_decision_tree tree = build_decision_tree(train_data)"]},{"cell_type":"markdown","metadata":{"id":"DNnGjMtKzsJ7"},"source":["# 3. Optimize Decision Tree with Algorithm Changes\n","\n","- Avoid calculating info gain for all possible splits (brute force)\n","- Instead, choose random subset of features to split on (reducing number of splits that need to be evaluted)\n","- Implemented by changing 2 functions: \n","    - generate_possible_splits\n","    - evaluate_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZ1Fb6ca0kvC"},"outputs":[],"source":["## NEW: generate random binary splits of the data ###\n","# Randomly sample a subset of the features to split on #\n","# This reduces the number of splits that need to be evaluated, #\n","# but still allows the algorithm to find a diverse range of splits. #\n","def generate_possible_splits(data, num_features, num_values):\n","    features = list(data.columns)[:-1]\n","    # WITH THIS LINE BELOW\n","    features = list(np.random.choice(features, num_features, replace=False))\n","    splits = []\n","    for feature in features:\n","        values = data[feature].unique()\n","        values = list(np.random.choice(values, min(num_values, len(values)), replace=False))\n","        for i in range(1, len(values)):\n","            for split in itertools.combinations(values, i):\n","                splits.append((feature, split))\n","    return splits\n","\n","# Avoid calculating the information gain for all evaluation splits #\n","def evaluate_split(split, data):\n","    # evaluate a split by calculating the information gain\n","    feature, values = split\n","    total = len(data)\n","    left = data[data[feature].isin(values)]\n","    right = data[~data[feature].isin(values)]\n","    # WITH THIS LINE BELOW #\n","    if len(left) == 0 or len(right) == 0:\n","        return -float('inf'), None, None\n","    left_proportion = len(left) / total\n","    right_proportion = len(right) / total\n","    entropy_before = calculate_entropy(data)\n","    entropy_after = (left_proportion * calculate_entropy(left) +\n","                     right_proportion * calculate_entropy(right))\n","    information_gain = entropy_before - entropy_after\n","    return information_gain, left, right\n","\n","# Update calls to generate_possible_splits to have the \n","# correct inputs (adding num_features and num_values)\n","def build_decision_tree(data):\n","    # build a decision tree using a brute force algorithm\n","    if len(data.iloc[:, -1].unique()) == 1:\n","        # all instances have the same class\n","        return {'class': data.iloc[0, -1]}\n","    best_split = None\n","    best_gain = 0\n","    for split in generate_possible_splits(data, num_features=5, num_values=3):\n","        gain, left, right = evaluate_split(split, data)\n","        if gain > best_gain:\n","            best_gain = gain\n","            best_split = split\n","            best_left = left\n","            best_right = right\n","    if best_split is None:\n","        # no split improved information gain\n","        return {'class': data.iloc[:, -1].value_counts().idxmax()}\n","    else:\n","        left_subtree = build_decision_tree(best_left)\n","        right_subtree = build_decision_tree(best_right)\n","        return {'feature': best_split[0],\n","                'value': best_split[1],\n","                'left': left_subtree,\n","                'right': right_subtree}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1hkjyM0zsJ7","outputId":"c3bd7553-ac44-47ad-9cb1-29a572530b9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.5964579690801155\n","Accuracy: 0.6250379516243295\n","Time: 56.65168905258179\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n"]}],"source":["evaluate()"]},{"cell_type":"markdown","metadata":{"id":"WIzsZivOzsJ8"},"source":["# 4. Optimize Decision Trees with Threading for evaluate_split Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMmVTHF8zsJ8"},"outputs":[],"source":["def evaluate_split(split, data, num_threads=4):\n","    # evaluate a split by calculating the information gain\n","    feature, values = split\n","    total = len(data)\n","    left = data[data[feature].isin(values)]\n","    right = data[~data[feature].isin(values)]\n","    # WITH THIS LINE BELOW #\n","    if len(left) == 0 or len(right) == 0:\n","        return -float('inf'), None, None\n","    left_proportion = len(left) / total\n","    right_proportion = len(right) / total\n","    \n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        futures = [executor.submit(calculate_entropy, subset) for subset in [data, left, right]]\n","        results = [f.result() for f in futures]\n","    \n","    entropy_before = results[0]\n","    entropy_after = (left_proportion * results[1] +\n","                     right_proportion * results[2])\n","    information_gain = entropy_before - entropy_after\n","    return information_gain, left, right"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1javVz46zsJ8","outputId":"0c9d1913-4481-48bd-c9d8-758b180fcfaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.5960389750233661\n","Accuracy: 0.622305434672604\n","Time: 70.43838691711426\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n"]}],"source":["evaluate()"]},{"cell_type":"markdown","metadata":{"id":"w8_3D0HwzsJ8"},"source":["# 5. Optimizing Model by Adding Variables for Number of Features and Number of Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlFGvG1RzsJ9"},"outputs":[],"source":["def generate_possible_splits(data, num_features, num_values):\n","    features = list(data.columns)[:-1]\n","    # WITH THIS LINE BELOW\n","    features = list(np.random.choice(features, num_features, replace=False))\n","    splits = []\n","    for feature in features:\n","        values = data[feature].unique()\n","        values = list(np.random.choice(values, min(num_values, len(values)), replace=False))\n","        for i in range(1, len(values)):\n","            for split in itertools.combinations(values, i):\n","                splits.append((feature, split))\n","    return splits\n","\n","# Avoid calculating the information gain for all evaluation splits\n","def evaluate_split(split, data):\n","    # evaluate a split by calculating the information gain\n","    feature, values = split\n","    total = len(data)\n","    left = data[data[feature].isin(values)]\n","    right = data[~data[feature].isin(values)]\n","    if len(left) == 0 or len(right) == 0:\n","        return None\n","    left_proportion = len(left) / total\n","    right_proportion = len(right) / total\n","    entropy_before = calculate_entropy(data)\n","    entropy_after = (left_proportion * calculate_entropy(left) +\n","                     right_proportion * calculate_entropy(right))\n","    information_gain = entropy_before - entropy_after\n","    if information_gain <= 0:\n","        return None\n","    return information_gain, left, right, split\n","\n","\n","def calculate_entropy(data):\n","    # calculate the entropy of a set of data\n","    total = len(data)\n","    counts = data.iloc[:, -1].value_counts()\n","    entropy = 0\n","    for count in counts:\n","        proportion = count / total\n","        entropy -= proportion * math.log2(proportion)\n","    return entropy\n","\n","\n","# Update calls to generate_possible_splits to have the \n","# correct inputs (adding num_features and num_values)\n","def build_decision_tree(data, num_features, num_values):\n","    # build a decision tree using a brute force algorithm\n","    if len(data.iloc[:, -1].unique()) == 1:\n","        # all instances have the same class\n","        return {'class': data.iloc[0, -1]}\n","    splits = generate_possible_splits(data, num_features, num_values)\n","    splits_gain = [split_gain for split_gain in (evaluate_split(split, data) for split in splits) if split_gain]\n","    if not splits_gain:\n","        return {'class': data.iloc[:, -1].value_counts().idxmax()}\n","    best_gain, best_left, best_right, best_split = max(splits_gain, key=lambda x: x[0])\n","    left_subtree = build_decision_tree(best_left, num_features, num_values)\n","    right_subtree = build_decision_tree(best_right, num_features, num_values)\n","    return {'feature': best_split[0],\n","            'value': best_split[1],\n","            'left': left_subtree,\n","            'right': right_subtree}\n","\n","\n","def predict(instance, tree):\n","    if 'class' in tree:\n","        # leaf node, return class\n","        return tree['class']\n","    else:\n","        feature = tree['feature']\n","        value = tree['value']\n","        if instance[feature] in value:\n","            subtree = tree['left']\n","        else:\n","            subtree = tree['right']\n","        return predict(instance, subtree)\n","    \n","def evaluate():\n","    # load dataset\n","    data = df\n","\n","    # split data into training and test sets\n","    train_data = data.sample(frac=0.7, random_state=1)\n","    test_data = data.drop(train_data.index)\n","\n","    # train decision tree\n","    start = time.time()\n","    tree = build_decision_tree(train_data, num_features=3, num_values=3)\n","    end = time.time()\n","    time_diff = end - start\n","\n","    # evaluate decision tree\n","    y_true = test_data.iloc[:, -1]\n","    y_pred = test_data.apply(lambda x: predict(x, tree), axis=1)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred))\n","\n","    # print results\n","    print(f'AUC: {auc}')\n","    print(f'Accuracy: {accuracy}')\n","    print(f'Time: {time_diff}')\n","    \n","    # Make beeping sounds to notify end of evaluation\n","    beep = lambda x: os.system(\"echo '\\a';sleep 0.5;\" * x)\n","    beep(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKm_pIoHzsJ9","outputId":"61a5415f-09e4-4f21-bbc8-08291d082c76"},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC: 0.6079748258819964\n","Accuracy: 0.6395101710353203\n","Time: 25.417531967163086\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n","\u0007\n"]}],"source":["evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1fEyH7vzsJ-"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}