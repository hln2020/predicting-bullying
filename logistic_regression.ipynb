{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1682784585965,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "A3Rx5bOToH8f"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import numba as nb\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import wait\n",
    "from concurrent.futures import as_completed\n",
    "from joblib import Parallel, delayed\n",
    "from numba import njit\n",
    "from numba import jit\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2145,
     "status": "ok",
     "timestamp": 1682784612050,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "pR4Vz03RoQUS"
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "df = pd.read_csv('cleaned_bullying.csv')\n",
    "\n",
    "# Drop columns\n",
    "df = df.drop(columns=['Unnamed: 0',\n",
    "                      'Bullied_on_school_property_in_past_12_months',\n",
    "                      'Bullied_not_on_school_property_in_past_12_months',\n",
    "                      'Cyber_bullied_in_past_12_months'])\n",
    "\n",
    "# Split into X and y\n",
    "X = df.drop('bullied', axis=1).values\n",
    "y = df['bullied'].values\n",
    "\n",
    "# Load copy of dataframe\n",
    "data = df\n",
    "\n",
    "# Train/test split\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for learning rate, number of iterations, and batch size\n",
    "learning_rate = 0.0001\n",
    "number_iterations = 100_000\n",
    "batch_num = 50\n",
    "digits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1682715969414,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "6RS4HEg3ptuQ"
   },
   "source": [
    "# 1. Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "    \n",
    "# Function for training model and making predictions\n",
    "def run_logistic_regression():\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    # Run and time model\n",
    "    start = time.time()\n",
    "    y_pred = run_logistic_regression()\n",
    "    end = time.time()\n",
    "    time_diff = round(end - start, digits)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    auc = round(roc_auc_score(y_test, y_pred), digits)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), digits)\n",
    "    precision = round(precision_score(y_test, y_pred), digits)\n",
    "    recall = round(recall_score(y_test, y_pred), digits)\n",
    "    f1 = round(f1_score(y_test, y_pred), digits)\n",
    "\n",
    "    # Print Results\n",
    "    print(f'Time: {time_diff}')\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'AUC: {recall}')\n",
    "    print(f'F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 57.3179\n",
      "AUC: 0.62521\n",
      "Accuracy: 0.67092\n",
      "Precision: 0.63946\n",
      "AUC: 0.39985\n",
      "F1: 0.49203\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimize Using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "    # optimized random batch selection\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        rng = np.random.default_rng()\n",
    "        idx = rng.integers(len(X), size=(self.num_iter, 10))\n",
    "        for i in range(self.num_iter):\n",
    "            batch_idx = idx[i]\n",
    "            batch_X = X[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            z = np.dot(batch_X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "    \n",
    "def run_logistic_regression():\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations,\n",
    "                            batch_size=batch_num)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.68186\n",
      "AUC: 0.6249\n",
      "Accuracy: 0.67061\n",
      "Precision: 0.63886\n",
      "AUC: 0.39947\n",
      "F1: 0.49157\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimize By Normalizing Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __normalize(self, X):\n",
    "        X_norm = X.copy()\n",
    "        self.mean = np.mean(X_norm, axis=0)\n",
    "        self.std = np.std(X_norm, axis=0)\n",
    "        X_norm = (X_norm - self.mean) / (self.std + 1e-8)\n",
    "        return X_norm\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    # optimized random batch selection\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Normalize the input features\n",
    "        X = self.__normalize(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        rng = np.random.default_rng()\n",
    "        idx = rng.integers(len(X), size=(self.num_iter, 10))\n",
    "        for i in range(self.num_iter):\n",
    "            batch_idx = idx[i]\n",
    "            batch_X = X[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            z = np.dot(batch_X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "\n",
    "def run_logistic_regression():\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations,\n",
    "                            batch_size=batch_num)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.84188\n",
      "AUC: 0.62838\n",
      "Accuracy: 0.6697\n",
      "Precision: 0.6264\n",
      "AUC: 0.4246\n",
      "F1: 0.50613\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize with Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5, num_threads=4):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_threads = num_threads\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "    def __batch_gradient_descent(self, batch_X, batch_y):\n",
    "        z = np.dot(batch_X, self.theta)\n",
    "        h = self.__sigmoid(z)\n",
    "        gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "        self.theta -= self.lr * gradient\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        # Create thread pool\n",
    "        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "            for i in range(self.num_iter):\n",
    "                # Generate random batch indices\n",
    "                batch_idx = np.random.randint(len(X) - self.batch_size + 1, size=self.num_threads)\n",
    "                batch_idx = [idx + j for j in range(self.batch_size) for idx in batch_idx]\n",
    "\n",
    "                # Split data into batches\n",
    "                batches_X = np.split(X[batch_idx], self.num_threads)\n",
    "                batches_y = np.split(y[batch_idx], self.num_threads)\n",
    "\n",
    "                # Submit batches for processing\n",
    "                futures = []\n",
    "                for j in range(self.num_threads):\n",
    "                    futures.append(executor.submit(self.__batch_gradient_descent, batches_X[j], batches_y[j]))\n",
    "\n",
    "                # Wait for all worker threads to finish\n",
    "                for future in futures:\n",
    "                    future.result()\n",
    "\n",
    "                if i % 10000 == 0:\n",
    "                    z = np.dot(X, self.theta)\n",
    "                    h = self.__sigmoid(z)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36.25018\n",
      "AUC: 0.62579\n",
      "Accuracy: 0.67122\n",
      "Precision: 0.63939\n",
      "AUC: 0.40175\n",
      "F1: 0.49345\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimize with Numba / CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5, verbose=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    # GD using numba\n",
    "    def fit_numba(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        self.theta = self.gradient_descent(X, y, np.zeros(X.shape[1]), self.lr, self.num_iter)\n",
    "        \n",
    "    @staticmethod    \n",
    "    @jit(nopython=True)\n",
    "    def gradient_descent(X, y, theta, lr, num_iter):\n",
    "        for i in range(num_iter):\n",
    "            z = np.dot(X, theta)\n",
    "            h = 1/(1+np.exp(-z))\n",
    "            gradient = np.dot(X.T, (h-y))/y.size\n",
    "            theta -= lr*gradient\n",
    "            \n",
    "        return theta\n",
    "    \n",
    "    # computing mini-batch gradient\n",
    "    @staticmethod    \n",
    "    @jit(nopython=True)\n",
    "    def SGD(X, y, theta, lr, num_iter, batch_size, idx):\n",
    "        for i in range(num_iter):\n",
    "            batch_idx = idx[i]\n",
    "            batch_X = X[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            z = np.dot(batch_X, theta)\n",
    "            h = 1/(1+np.exp(-z))\n",
    "            gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "            theta -= lr * gradient\n",
    "            \n",
    "        return theta\n",
    "\n",
    "    # use cuda kernel to do matrix multiplication\n",
    "    def fit_cuda(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        TPB=16\n",
    "        \n",
    "        X_d = cuda.to_device(np.float32(X))\n",
    "        y_d = cuda.to_device(y.reshape([y.shape[0],1]))\n",
    "        z_d = cuda.to_device(np.zeros([X.shape[0],1], dtype=np.float32))\n",
    "        h_d = cuda.to_device(np.zeros(z_d.shape, dtype=np.float32))\n",
    "        \n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        theta_d = cuda.to_device(np.zeros((X.shape[1],1), dtype=np.float32))\n",
    "        \n",
    "        threadsperblock = (TPB, TPB)\n",
    "        blockspergrid_x = math.ceil(z_h.shape[0] / threadsperblock[0])\n",
    "        blockspergrid_y = math.ceil(z_h.shape[1] / threadsperblock[1])\n",
    "        blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.num_iter):\n",
    "            #cuda_dot[blockspergrid, threadsperblock](X, theta_d, z_d)\n",
    "            fast_matmul[blockspergrid, threadsperblock](X, theta_d, z_d)\n",
    "            #cuda_sigmoid[blockspergrid, threadsperblock](z_d, h_d)\n",
    "            #h = h_d.copy_to_host()\n",
    "            z = z_d.copy_to_host()\n",
    "            h = self.__sigmoid(z)\n",
    "            \n",
    "            gradient = np.dot(X.T, (h.reshape(y.shape) - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if self.verbose:\n",
    "                if i % 10000 == 0:\n",
    "                    z = np.dot(X, self.theta)\n",
    "                    h = self.__sigmoid(z)\n",
    "                    print(f'Step: {i}')\n",
    "                    print(f'loss: {self.__loss(h, y)}')\n",
    "                    \n",
    "    def fit_optimized_numba(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        rng = np.random.default_rng()\n",
    "        idx = rng.integers(len(X), size=(self.num_iter, 10))\n",
    "        self.theta = self.SGD(X, y, np.zeros(X.shape[1]), self.lr, self.num_iter, self.batch_size, idx)\n",
    "    \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cuda_dot(A, B, C):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        C[i, j] = 0\n",
    "        for k in range(A.shape[1]):\n",
    "            C[i, j] += A[i, k] * B[k, j]\n",
    "        C[i,j] += C[i,j]\n",
    "\n",
    "@cuda.jit\n",
    "def cuda_sigmoid(z, out):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < z.shape[0] and j < z.shape[1]:\n",
    "        out[i, j] = 1 / (1 + math.exp(-z[i, j]))\n",
    "\n",
    "@cuda.jit\n",
    "def cuda_gradient(X, h, y, gradient):\n",
    "    i = cuda.grid(1)\n",
    "    if i < X.shape[1]:\n",
    "        gradient[i] = 0\n",
    "        for j in range(X.shape[0]):\n",
    "            gradient[i] += (h[j, 0] - y[j]) * X[j, i]\n",
    "        gradient[i] /= y.size\n",
    "\n",
    "@cuda.jit\n",
    "def cuda_subtract(a, b, diff):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < a.shape[0] and j < b.shape[1]:\n",
    "        diff[i,j] = a[i,j] - b[i,j]\n",
    "            \n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B using CUDA shared memory.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/64198479/13697228 by @RobertCrovella\n",
    "    \"\"\"\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=np.float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=np.float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = np.float32(0.)\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx + i * TPB) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * TPB]\n",
    "        if x < B.shape[1] and (ty + i * TPB) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * TPB, x]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression(version):\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations,\n",
    "                            batch_size=batch_num,\n",
    "                            verbose=False)\n",
    "    assert version in ['numba', 'optimized_numba', 'cuda']\n",
    "    if version == 'numba':\n",
    "        lr.fit_numba(X_train, y_train)\n",
    "    elif version == 'optimized_numba':\n",
    "        lr.fit_optimized_numba(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def evaluate(version):\n",
    "    \n",
    "    # Run and time model\n",
    "    start = time.time()\n",
    "    y_pred = run_logistic_regression(version)\n",
    "    end = time.time()\n",
    "    time_diff = round(end - start, digits)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    auc = round(roc_auc_score(y_test, y_pred), digits)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), digits)\n",
    "    precision = round(precision_score(y_test, y_pred), digits)\n",
    "    recall = round(recall_score(y_test, y_pred), digits)\n",
    "    f1 = round(f1_score(y_test, y_pred), digits)\n",
    "\n",
    "    # Print Results\n",
    "    print(f'Time: {time_diff}')\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'AUC: {recall}')\n",
    "    print(f'F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45.98526\n",
      "AUC: 0.62521\n",
      "Accuracy: 0.67092\n",
      "Precision: 0.63946\n",
      "AUC: 0.39985\n",
      "F1: 0.49203\n"
     ]
    }
   ],
   "source": [
    "evaluate('numba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.79876\n",
      "AUC: 0.62407\n",
      "Accuracy: 0.67046\n",
      "Precision: 0.64035\n",
      "AUC: 0.39528\n",
      "F1: 0.48882\n"
     ]
    }
   ],
   "source": [
    "evaluate('optimized_numba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: need CUDA GPU to run cuda model\n",
    "#evalute('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPrjaUf0u1X6u15lYYZoS4H",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
